[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Modeling in Stata",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#how-to-use-this-document",
    "href": "index.html#how-to-use-this-document",
    "title": "Regression Modeling in Stata",
    "section": "How to use this document",
    "text": "How to use this document\nThese notes are published using Quarto. The Stata code is first rendered using a Stata dynamic document. The source code for these notes can be found at https://github.com/josherrickson/stata-regression for the curious.\nAll images should link to full-size versions to see detail if needed."
  },
  {
    "objectID": "index.html#cscar",
    "href": "index.html#cscar",
    "title": "Regression Modeling in Stata",
    "section": "CSCAR",
    "text": "CSCAR\nhttp://cscar.research.umich.edu/\nCSCAR is available for free consultations with PhD statisticians (email deskpeople@umich.edu to request a consultation).\nCSCAR also has GSRAs available for more immediate help. Walk-ins to our office in Rackham are welcomed Monday-Friday 9am to 5pm (Closed Tuesdays 12-1pm). Alternatively, on our website, you can self-schedule into an hour consultation with the graduate students, which can be either remote or in-person (these are usually available same-day or next-day).\nCSCAR operates a email for help with statistical questions, feel free to send concise questions to stats-consulting@umich.edu.\nThe current contact for questions about the notes: Josh Errickson (jerrick@umich.edu)."
  },
  {
    "objectID": "01-regression.html",
    "href": "01-regression.html",
    "title": "1  Ordinary Least Squares",
    "section": "",
    "text": "2 Terminology\nWhen discussing any form of regression, we think of predicting the value of one variable1 based upon several other variables.\nThe variable we are predicting can be called the “outcome”, the “response” or the “dependent variable”.\nThe variables upon which we are predicting can be called “predictors”, “covariates”, or “independent variables”.\nThe model we’re going to start discussing is called “linear regression”. You may also have heard this called “least squares regression” or “ordinary least squares (OLS)”. A lot of the time, if you see a reference to “regression” without specifying the type, they are referring to linear regression.\nOrdinary Least Squares regression is the most basic form of regression. This is suitable for situations where you have some number of predictor variables and the goal is to establish a linear equation which predicts a continuous outcome. Technically the outcome need not be continuous, but there are often better forms of regression to use for non-continuous outcomes. The term “linear equation” refers back to high school geometry and the equation for a line,\n\\[\n    y = mx + b\n\\]\nIn that framing, the value of \\(y\\) can be obtained for a given value of \\(x\\), based upon the slope (\\(m\\)) and intercept (\\(b\\)). You can easily extend this to higher dimensions,\n\\[\n    y = mx + nz + b\n\\]\nNow the value of \\(y\\) also depends on \\(z\\) and it’s slope (\\(n\\)).\nLinear regression fits a model based on this equation of a line:\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p + \\epsilon\n\\]\nFor example, the relationship between the weight of a car (in lbs) and the length of a car (in inches) is approximately2:\n\\[\n    \\textrm{weight} = -3000 + 33*\\textrm{length}\n\\]\nThe intercept is meaningless - how many 0 length cars do you know? If we plug in a reasonable value for length, say 200 inches, we can solve for weight:\n\\[\n    \\textrm{weight} = -3000 + 33*200 = 3600\n\\]\nThis is the predicted weight; for a given car of length 200, it won’t be exactly 3600 lbs, but that difference is error (\\(\\epsilon\\)).\nStata can be used to estimate the regression coefficients in a model like the one above, and perform statistical tests of the null hypothesis that the coefficients are equal to zero (and thus that predictor variables are not important in explaining the response). Note that the response \\(Y\\) is modeled as a linear combination of the predictors and their coefficients.\nSome introductory statistical classes distinguish between simple regression (with only a single predictor) and multiple regression (with more than one predictor). While this is useful for developing the theory of regression, simple regression is not commonly used for real analysis, as it ignores one of the main benefits of regression, controlling for other predictors (to be discussed later).\nWe will now fit a model, discussing assumptions afterwards, because almost all assumption checks can only occur once the model is fit!\nFor demonstration purposes, we’ll use the 2015 Residential Energy Consumption Survey (RECS). This is a public data-set made available by the governmental Energy Information Administration containing household level information about “energy characteristics on the housing unit, usage patterns, and household demographics”3.\nLet’s fit a model predicting dollar expenditure on electricity (dollarel) based upon the square footage of the house (totsqft_en) and the gender of the respondent (hhsex). Before we fit the model, we want to briefly explore each variable involved.\nWe see here that expenditure on electricity ranges from under $ 20 to over $ 8000, though the vast majority seem to be between a few hundred dollars and $ 2000-3000. We will discuss a bit later approaches for dealing with right-skewed data.\nLooks very similar to dollarel.\nFor looking at the codebook online (Excel file downloadable here), we see that 1 is female and 2 is male. Let’s generate a new variable to make this distinction more clear\nNow female will be a 1 if the respondent is female and 0 otherwise.\nStata’s regress command fit the linear regression model. The general syntax is\nSo we fit the model:\nThere is a lot of important output here, so we will step through each piece.\nFirst, the top left table is the ANOVA table. If you were to fit a regression model with a single categorical predictor, this would be identical to running ANOVA via oneway. In general we don’t need to interpret anything here, as there are further measures of model fit in the regression frameworks.\nNext, the top right part has a series of measures.\nFinally, we get to the coefficient table. Each row represents a single predictor. The _cons row is the intercept; it’s Coef. of 880.27 represents the average response when all other predictors are 0. Given that square-footage cannot be 0, this is meaningless and can be ignored. (Note that we cannot exclude the constant, we are simply ignoring it.)\nWhenever we look at any model, a distinction needs to be drawn between statistical significance and practical significance. While these two interpretations of significance often align, they are not guaranteed to. We often have statistical significance (a p-value less than .05) when there is no practical significance (aka clinical significance, a difference that isn’t scientifically interesting). This is mostly a function of sample size; with a large sample even very small effects can have small p-values. Alternatively, a large practical significance with a low statistical significance can occur with very noisy data or a small sample size, which might indicate further study with a larger sample is needed.\nIn this example, the gender difference may qualify - the model is estimating that women respondents pay almost $10 more than men on average, however, this coefficient is not statistically distinguishable from 0. The confidence interval, ranging from -30 to 50, it extremely wide. It’s possible that there is a different between the genders of the respondent, but the estimate is small and noisy - and our sample size is not sufficient. All we can conclusively say is that we do not have enough evidence to claim there is a difference in gender.\nLet’s say we want to add some location information to the model. There is a variable (regionc) that identifies which region (Northeast, Midwest, South, West) the respondent lives in. It’s reasonable to test whether the energy expenditures differs by region, regardless of the size of the home. Let’s naively add it to the model.\nWe only get a single coefficient. What is the interpretation of it? There is none. Stata is treating region as continuous. Regression models cannot use categorical predictors. Instead, the regression model requires a series a dummy variables (e.g. northwest - Is this respondent in the Northwest? south - Is this respondent in the South?) for which each respondent has a single positive (1) response and the remainder negative (0) responses.\nWhile we could create this ourselves7, something we’d likely have to do in a software like SPSS, but Stata (and most modern software) can handle this automatically. The issue is that Stata doesn’t know we want to treat regionc as categorical. If we prefix the variable name with i., Stata will know it is categorical.\nFirst, let’s look at the regions and add appropriate labels from the codebook.\n(Note: Older versions of Stata require prefacing the command with xi: before it would recognize the i., e.g. xi: regress dollarel totsqft_en female i.regionc. xi: was deprecated a number of years ago, but you may see this used in older code online or by users who do not stay up to date.)\nThis model is improved, considering the Adjusted \\(R^2\\) values. The coefficient (and significance) on square footage is not really changed, but notice the magnitude of the coefficient on female has changed drastically. This implies that gender and region were correlated, and when we did not control for region, gender was including it’s effect. We will discuss multicollinearity later, as well as why this is one reason why model selection is bad.\nNow we see 3 rows for regionc, each corresponding to a comparison between region “Northeast” and the given row. When we include a categorical variable, one group is excluded as the baseline. By default, Stata removes the first group (here 1, Northeast). So we can see that the Midwest compared to the northeast has statistically significantly lower expenditure, with an average reduction of $ 217.85. Those in the South have higher average expenditure compared to Northeast, and those in the West have lower average expenditure.\nWe do not see a comparison of, for example, South versus West. To see the other comparisons we can use the margins command.\nThe first margins call, without any options, displays the marginal means for each category - if every respondent was from the given region, what is the average predicted expenditure. The t-test here is useless - it’s only testing that the average expenditure is non-zero. We see that the lowest average expenditure is in the Midwest, the highest in the South.\nThe second margins call adds the pwcompare(pv) option, which performs pairwise test between each pair of regions. This is similar to a post-hoc test from ANOVA if you are familiar with it. All regions are statistically significant from each other. Notice that the three comparisons to Northwest are identical to the results in the regression output.\nBy default, using i. makes the first level (lowest numerical value) as the reference category. You can adjust this by using ib#. instead, such as:\nThis does not fit a different model. Both models (with i.regionc and ib2.regionc) are identical, we’re just seeing slight variations. If the models do change (especially the model fit numbers in the top right), something has gone wrong. So what’s the point of this - well sometimes you only care about comparisons to a single baseline group. In that case, if you make that group the proper reference category, you don’t need to use margins.\nEach coefficient we’ve look at so far is only testing whether there is a relationship between the predictor and response when the other predictors are held constant. What if we think the relationship changes based on the value of other predictors? For example, we might be interested in whether the relationship between square footage and expenditure differs by region.\nMathematically an interaction is nothing more than a literal multiplication. For example, if our model has only two predictors,\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\n\\]\nthen to add an interaction between \\(X_1\\) and \\(X_2\\), we simply add a new multiplicative term.\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1\\times X_2) + \\epsilon\n\\]\nAdding these to the regress call is almost as easy. We’ll use # or ## instead. # includes only the interaction, whereas ## includes both the interaction and the main effects.\nNote that we used c., similar to i.. c. forces Stata to treat it as continuous. Stata assumes anything in an interaction is categorical, so we need c. here! This can get pretty confusing, but it’s never wrong to include i. or c. when specifying a regression.\nOnce we include an interaction, the relationship between the variables included in the interaction and the response are not constant - the relationship depends on the value of the other interacted variables. This can be hard to visualize with the basic regression output, so we’ll look at margins again instead. We’ll want to look at the relationship between square footage and expenditure in each region.\nThe dydx() option specifies that instead of marginal means (as we had above), we want to look at marginal slopes - that is, the slope between square footage and expenditure in each region. Recall that without the interaction the coefficient associated with square footage was approximately .25. Here we see that in the South and West that relationship is actually steeper, while in the Northeast and Midwest it’s shallower. The t-tests are testing whether the slope in each region is significantly different than zero.\nNote that the slope in Northeast in the margins call is identical to the main effect of square footage. The margins command is not telling us anything we could not have obtained or calculated from the regression output - it’s just doing so with minimal effort and maximal clarity.\nLet’s test whether the slopes differ between regions.\nIt can sometimes be tricky to look at these tests and determine what it is telling us, but what this is basically saying is that Northeast and Midwest have the same slope, and South and West have the same slope. Then Northeast/Midwest is different than South/West. There’s a bit of confusion because West and Northeast are not statistically distinguishable (although .6 is extremely close to significance). This sort of thing happens often with multiple pairwise comparison, it’s best to try and focus on the overarching result instead of getting bogged down in details.\nWe can call margins with slightly different options to be able to produce an interaction plot. Rather than using the dydx option, we’ll use the at option to estimate marginal means at specific values of square footage.\nFollow this with a call to marginsplot:\nThere isn’t too much interesting here that we haven’t identified before. Often there is, and this plot will be useful. You can use the pwcompare(pv) option alongside the at() option to test for differences in region at specific values of square footage.\nThe standard error associated with each coefficient are determined with the assumption that the model is “true” and that, were we given an infinite sample size, the estimates \\(\\hat{\\beta}\\) would converge to the true \\(\\beta\\). In many situations, this is clearly untrue.\nIf you believe this is untrue, the estimates will be unaffected, but their standard errors will be incorrect. We can adjust for this by using “robust” standard errors, also known as Sandwich estimators or Huber-White estimators, with the vce(robust) option to regress.\nNotice that compared to the previous model, the coefficient estimates are identical but the standard errors (and corresponding t-statistic, p-value and confidence interval) are slightly different.\nTypically, the robust standard errors will be larger than the non-robust standard errors, but not always. Generally, the only situation where the robust standard errors will decrease is when the error variance is highest for observations near the average value of the predictors. This does not often happen (generally the higher residuals occur in observations that could be considered outliers).\nThere has been some argument that robust standard errors should always be used, because if the model is correctly specified, the robust standard errors and regular standard errors should be almost identical, so there is no harm in using them.\nThere are three main assumptions when running a linear regression. Some we can test, some we cannot (and need to rely on our knowledge of the data).\nSometimes if one of the above assumptions is violated, it can be addressed by a simple variable transformation. The most common is taking the log of a right-skewed variable.\nWe see a slight improvement to the model, though expenditure did not start out too skewed.\nOnce we make this transformation, the interpretation of the coefficients change. By exponentiating the coefficients, we can determine the new interpretation. (Calling regress or any estimation command after running one replays the results without having to re-calculate them.)\nThe string in quotes (\"Exp(Coef.)\") is only for display purposes; eform is the important option. Now you can interpret each coefficient as the percent change in the outcome for a 1-unit increase in the predictor. For example, a coefficient of 1.15 would indicate that for a 1-unit increase in x, you predict an average increase of 15% in y. A coefficient of .89 would predict an average decrease of 11% in y."
  },
  {
    "objectID": "01-regression.html#centering",
    "href": "01-regression.html#centering",
    "title": "1  Ordinary Least Squares",
    "section": "6.1 Centering",
    "text": "6.1 Centering\nSome sources suggest centering continuous predictors before including them in an interaction. This will change the coefficients in the regression output, but will not fit a different model. What may be useful is the main effects of terms involved in the interaction are now when the other variable is at it’s mean, rather than at 0. For example, in the model above with the interaction of square footage and region, the coefficients on region are the differences in region when square footage is 0 - not interesting. If we centered square footage, then the coefficients on region would be testing for differences when square footage is at it’s mean.\nHowever, once again, this is not fitting a different model. The results will be identical. I’d always recommend looking at the margins and interaction plot, even if you do center."
  },
  {
    "objectID": "01-regression.html#fitting-separate-models",
    "href": "01-regression.html#fitting-separate-models",
    "title": "1  Ordinary Least Squares",
    "section": "6.2 Fitting Separate Models",
    "text": "6.2 Fitting Separate Models\nA natural instinct when asking the question “does the relationship between X and Y differ by group” would be to fit separate models for each group. This is equivalent to interacting group with every predictor in the model.\nThe downside of this approach is that each individual model is less powerful that fitting one overall model.\nThe upside is that the interpretation may be a bit easier.\nIf you go this approach, you can use the estimates store commands to save each model, then use lrtest to compare models (test whether all coefficients are equal between models) or use suest with test to test individual coefficients. See for example this UCLA page."
  },
  {
    "objectID": "01-regression.html#relationship-is-linear-and-additive",
    "href": "01-regression.html#relationship-is-linear-and-additive",
    "title": "1  Ordinary Least Squares",
    "section": "8.1 Relationship is linear and additive",
    "text": "8.1 Relationship is linear and additive\nRecall the linear regression model:\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p + \\epsilon\n\\]\nThis very explicitly assumes that the relationship is linear (as opposed to something non-linear, such as quadratic or exponential) and additive (as opposed to multiplicative). We can examine this assumption by looking at plots of the residuals (estimated errors):\n. rvfplot\n\nWhat we’re seeing here is a scatterplot between the fitted values (the predicted values for each individual) and their errors (the difference between the predicted values and observed values). If you can see a pattern in the scatterplot, that is evidence that this assumption is violated. Importantly, not seeing any pattern is not evidence that the assumption is valid! You’ll still need to cover this assumption with theory and knowledge of the data.\nThis image, from Julian Faraway’s Linear Models with R book, demonstrates a lack of pattern (the first) and a pattern (the third). (We will discuss the second plot below).\n\nIf this assumption is violated, you will need to reconsider the structure in your model, perhaps by adding a squared term (e.g. reg y c.x c.x#c.x) or in this case, trying a log transformation.\n\n8.1.1 Obtaining predicted values and residuals\nIn the rvfplot, we plotted residuals versus predicted values - neither of which we have in the data. If there is some analysis beyond what rvfplot produces that you’re interested in, the predict command can obtain these. The general syntax for predict is:\npredict &lt;new var name&gt;, &lt;statistic&gt;\nThere are quite a few options for the “statistic”, but the two most commonly used ones are:\n\nxb: The linear prediction (also the default). This is the predicted value for each individual based on the model.\nresiduals: The residuals. The difference between the predicted value and observed value.\n\nIn other words, we can replicate the above rvfplot via:\n. predict linearpredictor, xb\n\n. predict resids, residuals\n\n. twoway scatter resids linearpredictor"
  },
  {
    "objectID": "01-regression.html#errors-are-homogeneous",
    "href": "01-regression.html#errors-are-homogeneous",
    "title": "1  Ordinary Least Squares",
    "section": "8.2 Errors are homogeneous",
    "text": "8.2 Errors are homogeneous\n“Homogeneity” is a fancy term for “uniform in distribution”, whereas “heterogeneity” represents “not uniform in distribution”. If we were to take a truly random sample of all individuals in Michigan, the distribution of their heights would be homogeneous - it is reasonable to assume there is only a single distribution at work there. If on the other hand, we took a random sample of basketball players and school children, this would definitely be heterogeneous. The basketball players have a markedly difference distribution of heights that school children!\nIn linear regression, the homogeneity assumption is that the distribution of the errors is uniform. Violations would include errors changing as the predictor increased, or several groups having very different noise in their measurements.\nThis is an assumption we can examine, again with the residuals vs fitted plot. We’re looking for either a blatant deviation from a mean of 0, or an increasing/decreasing variability on the y-axis over time. Refer back to the image above, looking at the middle plot. As the fitted values increase, the error spreads out.\nIf this assumption is violated, you may consider restructuring your model as above, or transforming either your response or predictors using log transforms."
  },
  {
    "objectID": "01-regression.html#independence",
    "href": "01-regression.html#independence",
    "title": "1  Ordinary Least Squares",
    "section": "8.3 Independence",
    "text": "8.3 Independence\nThe last assumption is that each row of your data is independent. If you have repeated measures, this is violated. If you have subjects drawn from groups (i.e. students in classrooms), this is violated. There is no way to test for this, it requires knowing the data set.\nThis is, by far, the most important assumption. If this assumption is violated, some sort of repeated measures approach may be more appropriate such as mixed effects regression."
  },
  {
    "objectID": "01-regression.html#multicollinearity",
    "href": "01-regression.html#multicollinearity",
    "title": "1  Ordinary Least Squares",
    "section": "10.1 Multicollinearity",
    "text": "10.1 Multicollinearity\nMulticollinearity is an issue when 2 or more predictors are correlated. If only two are correlated, looking at their correlation (with pwcorr or correlate) may provide some indication, but you can have many-way multicollinearity where each pairwise correlation is low. You can use the variance inflation factor to try and identify if this is an issue.\n. estat vif\n\n    Variable |       VIF       1/VIF  \n-------------+----------------------\n  totsqft_en |      6.79    0.147185\n     regionc |\n          2  |      7.99    0.125149\n          3  |      8.67    0.115281\n          4  |      8.09    0.123608\n     regionc#|\nc.totsqft_en |\n          2  |      9.00    0.111056\n          3  |      8.89    0.112494\n          4  |      7.23    0.138282\n      female |      1.01    0.994243\n-------------+----------------------\n    Mean VIF |      7.21\nThe rule of thumb is VIF &gt; 10 or 1/VIF (called the tolerance) &lt; .1 suggests that the variable is involved in multicollinearity and more exploration may be needed.\nMulticollinearity can be an issue because the more correlated predictors are, the more likely that their combined effect will be inappropriately spread among them. For a very simple example, imagine that we have the model\n\\[\n  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\n\\]\nIf \\(X_1\\) and \\(X_2\\) are uncorrelated, then we can estimate \\(\\beta_1\\) and \\(\\beta_2\\) without a problem. Consider the extreme situations where \\(X_1\\) and \\(X_2\\) are perfectly correlated.8 We can therefore rewrite the equation as\n\\[\n  Y = \\beta_0 + (\\beta_1 + \\beta_2)X_1 + \\epsilon\n\\]\nsince with perfect correlation, \\(X_1\\) and \\(X_2\\) are identical.9 Now, when we fit the model, we would have estimates of \\(\\beta_1\\) and \\(\\beta_2\\) which sum to the “truth”, but the individual level of each of \\(\\beta_1\\) and \\(\\beta_2\\) could be anything. For example, if the “true” \\(\\beta_1\\) and \\(\\beta_2\\) are 1 and 3, they sum to 4. We could get estimated coefficients of 1 and 3, or 3 and 1, or -20 and 24!\nThis is an extreme example, but in practice we can be close to this situation."
  },
  {
    "objectID": "01-regression.html#overfitting",
    "href": "01-regression.html#overfitting",
    "title": "1  Ordinary Least Squares",
    "section": "10.2 Overfitting",
    "text": "10.2 Overfitting\nOverfitting occurs when a model includes so many predictors that you can no longer generalize to the population. The rule of thumb is that you should have no more than one predictor for every 10-20 observations. The smaller your sample size, the more conservative you should be. For example, a sample size of 100 should use no more than 10-20 predictors. Recall that a categorical predictor with \\(k\\) different levels adds \\(k-1\\) predictors!"
  },
  {
    "objectID": "01-regression.html#model-selection-is-bad",
    "href": "01-regression.html#model-selection-is-bad",
    "title": "1  Ordinary Least Squares",
    "section": "10.3 Model Selection is bad",
    "text": "10.3 Model Selection is bad\nThere is a literature on the idea of model selection, that is, an automated (or sometimes manual) way of testing many versions of a model with a different subset of the predictors in an attempt to find the model that fits best. These are sometimes called “stepwise” procedures.\nThis method has a number of flaws, including\n\nDoing this is basically “p-value mining”, that is, running a lot of tests till you find a p-value you like.\nYour likelihood of making a false positive is very high.\nAs we saw earlier, adding a new variable can have an effect on existing predictors.\n\nInstead of doing model selection, you should use your knowledge of the data to select a subset of the variables which are either a) of importance to you, b) theoretically influential on the outcome (e.g. demographic variables) or c) what others (reviewers) would think are influential on the outcome. Then you can fit a single model including all of this. The “subset” can be all predictors if the sample size is sufficient.\nNote that adjustments to fix assumptions (e.g. transformations) or multicollinearity would not fall into the category of model selection and are fine to use."
  },
  {
    "objectID": "01-regression.html#footnotes",
    "href": "01-regression.html#footnotes",
    "title": "1  Ordinary Least Squares",
    "section": "",
    "text": "There are variations of regression with multiple outcomes, but they are for very specialized circumstances and can generally be fit as several basic regression models instead.↩︎\nThis is based a database of cars from 1978, and with some pretty harsh rounding of the coefficients, for demonstration purposes.↩︎\nhttps://www.eia.gov/consumption/residential/about.php↩︎\nThere likely was, but the RECS does data imputation for you.↩︎\nThe 2 and 5683 are degrees of freedom. They don’t typically add any interpretation.↩︎\nThe only exception is if the predictor being added is either constant or identical to another variable, in which case Stata would drop the variable before fitting the model anyways..↩︎\nYou could use tab regionc, generate(region) to create it.↩︎\nNote that if you provide data with perfect correlation, Stata will drop one of them for you. This in only a thought exercise. If it helps, imagine their correlation is 99% instead of perfect, and add “almost” as a qualifier to most claims.↩︎\nTechnically there could be a scaling factors such that \\(X_1 = aX_2 + b\\), but let’s assume without loss of generality that \\(a=1\\) and \\(b=0\\).↩︎"
  },
  {
    "objectID": "02-glm.html#footnotes",
    "href": "02-glm.html#footnotes",
    "title": "2  Generalized Linear Models",
    "section": "",
    "text": "Technically that maximizes likelihood, but that distinction is not important for understanding.↩︎"
  },
  {
    "objectID": "02-glm.html#logistic-regression",
    "href": "02-glm.html#logistic-regression",
    "title": "2  Generalized Linear Models",
    "section": "2.1 Logistic Regression",
    "text": "2.1 Logistic Regression\nLogistic regression is used when the outcome is dichotomous - either a positive outcome (1) or a negative outcome (0). For example, presence or absence of some disease. The link function for logistic regression is logit,\n\\[\n    \\textrm{logit}(x) = \\textrm{log}\\Big(\\frac{x}{1-x}\\Big)\n\\]\n\\[\n    \\textrm{logit}\\left(P(Y = 1 | X)\\right) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p + \\epsilon.\n\\]\nNote also that unlike in OLS, the left-hand side is not the observed outcome, but rather the probability of a positive outcome. So the goal of a logistic model is not to predict whether an individual will have a positive outcome, but rather to predict their probability of a positive outcome. This is a subtle difference, but worth pointing out since predicted values will be probabilities, not a binary response.\n\n2.1.1 Fitting the logistic model\nWe can fit a logistic regression using the logit command in State. It works very similarly to regress. Let’s run a model predicting the presence of a cellar based on square footage, region and electricity expenditure.\n. tab cellar\n\n     CELLAR |      Freq.     Percent        Cum.\n------------+-----------------------------------\n         -2 |      1,455       25.59       25.59\n          0 |      2,490       43.79       69.38\n          1 |      1,741       30.62      100.00\n------------+-----------------------------------\n      Total |      5,686      100.00\n\n. replace cellar = . if cellar == -2\n(1,455 real changes made, 1,455 to missing)\n\n. logit cellar dollarel totsqft_en i.regionc female\n\nIteration 0:  Log likelihood = -2866.0585  \nIteration 1:  Log likelihood =      -1748  \nIteration 2:  Log likelihood = -1730.0798  \nIteration 3:  Log likelihood = -1729.9962  \nIteration 4:  Log likelihood = -1729.9962  \n\nLogistic regression                                    Number of obs =   4,231\n                                                       LR chi2(6)    = 2272.12\n                                                       Prob &gt; chi2   =  0.0000\nLog likelihood = -1729.9962                            Pseudo R2     =  0.3964\n\n------------------------------------------------------------------------------\n      cellar | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    dollarel |  -.0004174   .0000575    -7.26   0.000    -.0005301   -.0003048\n  totsqft_en |   .0009164   .0000417    21.98   0.000     .0008347    .0009981\n             |\n     regionc |\n    Midwest  |  -.3488001   .1461488    -2.39   0.017    -.6352464   -.0623538\n      South  |  -3.188248   .1435758   -22.21   0.000    -3.469651   -2.906844\n       West  |   -3.11719   .1482945   -21.02   0.000    -3.407842   -2.826538\n             |\n      female |  -.1458111   .0864176    -1.69   0.092    -.3151864    .0235642\n       _cons |   .0504945   .1659137     0.30   0.761    -.2746904    .3756795\n------------------------------------------------------------------------------\nWhen you try this yourself, you may notice that its not quite as fast as regress. That is because for OLS we have a “closed form solution” - we just do some quick math and reach an answer. However, almost every other type of regression lacks a closed form solution, so instead we solve it iteratively - Stata guesses at the best coefficients that minimize error1, and uses an algorithm to repeatedly improve those coefficients until the reduction in error is below some threshold.\nFrom this output, we get the Number of obs again. Instead of an ANOVA table with a F-statistic to test model significance, there is instead a chi2 (\\(\\chi^2\\), pronounced “ky-squared” as in “Kyle”). In this model, we reject the null that all coefficients are identically 0.\nWhen we move away from linear regression, we no longer get an \\(R^2\\) measure. There have been various pseudo-\\(R^2\\)’s suggested, and Stata reports one here, but be careful assigning too much meaning to it. It is not uncommon to get pseudo-\\(R^2\\) values that are negative or above 1. We’ll discuss measuring goodness of fit below.\nThe coefficients table is interpreted in almost the same way as with regression. We see that square footage and energy expenditure have significant coefficient (positive and negative respectively), and there appears to be no gender effect. There are differences between regions.\nHowever, we cannot nicely interpret these coefficients, which are known as the “log odds”. All we can say is that “As square footage increases, the probability of a house having a cellar increases.”\nTo add any interpretability to these coefficients, we should instead look at the odds ratios. These are the exponentiated log odds. We can ask Stata to produce these with the or option.\n. logit, or\n\nLogistic regression                                    Number of obs =   4,231\n                                                       LR chi2(6)    = 2272.12\n                                                       Prob &gt; chi2   =  0.0000\nLog likelihood = -1729.9962                            Pseudo R2     =  0.3964\n\n------------------------------------------------------------------------------\n      cellar | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    dollarel |   .9995826   .0000575    -7.26   0.000       .99947    .9996953\n  totsqft_en |   1.000917   .0000417    21.98   0.000     1.000835    1.000999\n             |\n     regionc |\n    Midwest  |   .7055342   .1031129    -2.39   0.017     .5298049    .9395505\n      South  |   .0412441   .0059217   -22.21   0.000     .0311279    .0546479\n       West  |   .0442814   .0065667   -21.02   0.000     .0331126    .0592175\n             |\n      female |    .864321   .0746925    -1.69   0.092     .7296529    1.023844\n       _cons |   1.051791   .1745066     0.30   0.761     .7598073     1.45598\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\nNotice that the chi2, Pseudo R2, z and P&gt;|z| do not change - we’re fitting the same model! We’re just changing how the coefficients are represented.\nOdds ratios null hypothesis is at 1, not at 0. A value of 1 represents equal odds (or no change in odds). Odds ratios are always positive. So a significant odds ratio will be away from 1, rather than away from 0 as in linear regression or the log odds. The interpretation of odds ratios can be tricky, so let’s be precise here.\nFor categorical predictors, the interpretation is fairly straightforward. The coefficient on females is 0.8643. This means that for every 1 female respondent who has a basement in their house, you would expect 0.8643 male respondents to have a basement.\nFor continuous predictors, its the odds as the value of the predictor changes. Consider the coefficient on energy expenditure, 0.999583. For every 1 house of expenditure \\(e\\) which has a cellar, you’d expect 0.999583 houses at expenditure \\(e+1\\) to have a cellar.\nThose coefficients are really close to 1 due to scaling: a $1 increase or 1-sqft increase is irrelevant. Due to the non-linear relationship between the predictors and the outcome, we cannot simply multiply the odds ratios. Instead, let’s scale the variables and re-fit the model.\n. generate dollarel1000 = dollarel/1000\n\n. generate totsqft1000 = totsqft_en/1000\n\n. logit cellar dollarel1000 totsqft1000 i.regionc female, or\n\nIteration 0:  Log likelihood = -2866.0585  \nIteration 1:  Log likelihood =      -1748  \nIteration 2:  Log likelihood = -1730.0798  \nIteration 3:  Log likelihood = -1729.9962  \nIteration 4:  Log likelihood = -1729.9962  \n\nLogistic regression                                    Number of obs =   4,231\n                                                       LR chi2(6)    = 2272.12\n                                                       Prob &gt; chi2   =  0.0000\nLog likelihood = -1729.9962                            Pseudo R2     =  0.3964\n\n------------------------------------------------------------------------------\n      cellar | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ndollarel1000 |   .6587254   .0378633    -7.26   0.000     .5885423    .7372778\n totsqft1000 |   2.500286   .1042513    21.98   0.000     2.304083    2.713196\n             |\n     regionc |\n    Midwest  |   .7055342   .1031129    -2.39   0.017     .5298049    .9395505\n      South  |   .0412441   .0059217   -22.21   0.000     .0311279    .0546479\n       West  |   .0442814   .0065667   -21.02   0.000     .0331126    .0592175\n             |\n      female |    .864321   .0746925    -1.69   0.092     .7296529    1.023844\n       _cons |   1.051791   .1745066     0.30   0.761     .7598073     1.45598\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\nNote once again that the model fit characteristics haven’t changed; we’ve fit the same model, just with different units. Now the interpretation are more reasonable. As the expenditure increases by $1000, the odds of having a cellar decrease by 34%.\nFor every additional 1000-square feet, the odds of having a cellar increases by 150%. In other words, for every two 2000-square foot house that have a cellar, you’d expect five 3000-square foot houses to have cellars.\n\n\n2.1.2 Categorical Variables and Interactions\nBoth categorical variables and interactions can be included as they were in linear regression, with the appropriate interpretation of coefficients/odds ratios.\n\n\n2.1.3 margins and predict\nThe margins command works mostly the same, though it produces results on the probability scale, not the odds scale.\n. margins regionc\n\nPredictive margins                                       Number of obs = 4,231\nModel VCE: OIM\n\nExpression: Pr(cellar), predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     regionc |\n  Northeast  |   .7852513   .0181353    43.30   0.000     .7497068    .8207958\n    Midwest  |   .7288181   .0144762    50.35   0.000     .7004453    .7571909\n      South  |   .2087012   .0102263    20.41   0.000     .1886581    .2287443\n       West  |    .218189   .0113706    19.19   0.000      .195903     .240475\n------------------------------------------------------------------------------\nThe predict command adds a new statistic. xb now is the linear predictor, which is often not useful. Instead, the pr statistic returns the estimated probability of a positive outcome.\n\n\n2.1.4 Logistic regression assumptions\nThe assumptions for logistic regression are simpler than linear. The outcome measure must be binary with a 0 for a negative response and 1 for a positive. (Technically they don’t have to be positive/negative. We could think of predicting male/female and code male = 0 and female = 1. However, the convention would be to consider the outcome as “The person is female” so a 1 represents a “success” and a 0 a “failure” of that statement.) The errors in a logistic regression model are fairly contained (you can’t be wrong than more than 1!) so there are no real assumptions about them. The independence assumption is still here and still important, again, a mixed logistic model may be appropriate if the data is not independent.\n\n\n2.1.5 Logistic goodness of fit\nAs we mentioned earlier, there are various issues with the Pseudo \\(R^2\\) reported by logit, so use it carefully. In fact, all measures of goodness of fit in non-OLS models are problematic. However, here are two approaches commonly used in logistic regression.\nThe first is to look at a classification results: If we were to choose a threshold (say .2) and classify positive/negative outcomes against it (If a predicted probability is below .2, classify as 0. If a predicted probability is above .2, classify as 1.). We don’t know what the “correct” threshold is (if you even believe there could be one), but we can test over a range of thresholds and see how well we classify.\n. lroc\n\nLogistic model for cellar\n\nNumber of observations =     4231\nArea under ROC curve   =   0.8928\n\nThis is called a ROC curve (Receiver Operating Characteristic). Starting with a threshold of 1 (so no houses are predicted to have a cellar) at (0,0) and continuing to a threshold of 0 (all houses are predicted to have a cellar), each point is plotted as sensitivity (percent of correctly predicted positive responses) versus specificity (incorrecly predicted negative responses). With a threshold of 1, every house with a cellar is predicted to not a cellar (so 0% correct) and every house without a cellar is predicted to have a cellar (0% correct). As the threshold increases, these values are computed and plotted. The diagonal \\(y=x\\) line represents a completely uninformative model, and the ROC curve cannot pass below it. The closer it is to the top left corner, the better predictive the model is. The AUC (area under curve) is another measure of model-fit: .5 would indicate no information (ROC on the diagonal), 1 would indicate perfect fit (ROC to the top left corner). The AUC here is 0.89 indicating some predictive power.\nThe second is a more formal test. There are two variants, a Pearson \\(\\chi^2\\) test and the Hosmer-Lemeshow test. Both are fit with the estat gof command. Both are testing the hypothesis that the observed positive responses match predicted positive responses in subgroups of the population. Therefore we do not want to reject these tests, and a large p-value is desired.\n. estat gof\n\nGoodness-of-fit test after logistic model\nVariable: cellar\n\n      Number of observations =   4,231\nNumber of covariate patterns =   4,231\n          Pearson chi2(4224) = 4070.48\n                 Prob &gt; chi2 =  0.9539\nWe see here a p-value of 0.954, failing to reject the null, so there is no evidence that the model fits poorly.\nThere is some concern that when the “number of covariate patterns” is close to the number of observations , the Pearson test is invalid. In this data, we see that 4231 is indeed equal to 4231. Instead, we can use the Hosmer-Lemeshow by passing the group(#) option:\n. estat gof, group(10)\nnote: obs collapsed on 10 quantiles of estimated probabilities.\n\nGoodness-of-fit test after logistic model\nVariable: cellar\n\n Number of observations =  4,231\n       Number of groups =     10\nHosmer–Lemeshow chi2(8) =  58.12\n            Prob &gt; chi2 = 0.0000\nThe p-value gets very significant.\nWhy did we choose 10 groups? It’s just the standard. The only thing to keep in mind is that the Hosmer-Lemeshow test is only appropriate if the number of groups is greater than the number of predictors (including the intercept). In this model, we had two predictors, so that’s 3 total (including the intercept), so 10 is OK. There is some discussion that this choice can bias results - there are examples out there where passing 9 to the option rejects the test, whereas passing 11 passes.\nOverall, as mentioned, take these goodness-of-fit measures with a grain of salt. Focus on interpreting the coefficients.\n\n\n2.1.6 Separation\nSince the logistic regression model is solved iteratively, this can fail for a number of reasons. Before you begin interpreting the model, you’ll want to glance at the iteration steps and make sure that no errors were printed. The most common failure is due to separation.\nWith a binary outcome instead of a continuous outcome, it is much easier to have a predictor (or set of predictors) that perfectly predict the outcome. Consider trying to predict gender based on height. With a smaller sample, it’s not hard to imagine a scenario where every male is taller than every female. This is called “perfect separation”; using this sample, knowing height gives perfect information about gender\n“Partial separation” can also occur; this is when prediction is perfect only for one limit. Take the height scenario again; say everyone above 5’8” is male, and there are two men but the rest women below 5’8”. Here, we will always predict Male for heights above 5’8”.\nSeparation (of either type) often produces coefficients to be extreme with large standard errors. Stata will sometimes warn about this, but not always. If you notice these exceptional coefficients or if Stata does warn about separation, you’ll need to investigate and consider excluding certain predictors. It may seem counter-intuitive to exclude extremely highly predictive variables, but if a variable produces perfect separation, you don’t need a model to inform you of that.\n\n\n2.1.7 logit Miscellaneous.\nThe logit model supports the margins command just like regress does.\nIt does not support estat vif, but you can re-run the model as with regress as the VIF does not depend on the outcome. E.g.,\nlogit y a b c\nregress y a b c\nestat vif\nCollinearity, overfitting, and model selection remain concerns in the logistic model.\nRobust standard errors via vce(robust) are supported.\nOne other common cause of failed convergence is scaling - try scaling all your variables and see if that improves convergence.\n\n\n2.1.8 logit vs logistic\nInstead of logit, we could run the logistic command. The only difference is that logistic reports the odds ratio by default whereas logit reports the log odds. My personal preference is logit, but there’s no need to use one over the other.\n\n\n2.1.9 Sample size concerns\nWhen the percent of positive outcomes is close to 50%, the rules we discussed for OLS hold, 10-20 observations per predictor. As the percent of positive outcomes deviates from 50%, you may need a much larger sample size - mostly to ensure a reasonable number of both positive and negative outcomes. For example, if you expect 5% of individuals to have a positive outcome, and have a sample size of 40, that’s only 2 individuals with a positive outcome! Instead you should strive to have at least 10 or ideally over 100 individuals per outcome.\n\n\n2.1.10 Rare outcomes\nIf the percent of positive outcomes is extreme (e.g. 99% or 1%), logistic regression may fail to converge. Sometimes Poisson regression which we’ll talk about next can be used in this situation. The estimated coefficients will be slightly biased, but convergence may easier to achieve."
  },
  {
    "objectID": "02-glm.html#poisson-regression",
    "href": "02-glm.html#poisson-regression",
    "title": "2  Generalized Linear Models",
    "section": "2.2 Poisson regression",
    "text": "2.2 Poisson regression\nWhen the response variable is a count (number of occurences), then using a log link function produces what’s known as Poisson regression.\n\n2.2.1 Fitting the model\nAs with logistic regression, the Poisson regression command is simple and similar to regress. Let’s predict the number of rooms in a house based upon the variables we’ve been dealing with so far.\n. histogram totrooms\n(bin=37, start=1, width=.48648649)\n\n. poisson totrooms dollarel1000 totsqft1000 i.cellar i.regionc female\n\nIteration 0:  Log likelihood = -8755.2204  \nIteration 1:  Log likelihood = -8755.2185  \nIteration 2:  Log likelihood = -8755.2185  \n\nPoisson regression                                      Number of obs =  4,231\n                                                        LR chi2(7)    = 986.09\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -8755.2185                             Pseudo R2     = 0.0533\n\n------------------------------------------------------------------------------\n    totrooms | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ndollarel1000 |   .0576783    .007261     7.94   0.000      .043447    .0719096\n totsqft1000 |   .1124977   .0049483    22.73   0.000     .1027992    .1221962\n    1.cellar |   .0609588   .0159101     3.83   0.000     .0297756    .0921421\n             |\n     regionc |\n    Midwest  |   .0242295    .019944     1.21   0.224      -.01486    .0633189\n      South  |   .0369115   .0211105     1.75   0.080    -.0044642    .0782873\n       West  |   .0705931   .0218298     3.23   0.001     .0278075    .1133788\n             |\n      female |  -.0206701   .0117413    -1.76   0.078    -.0436825    .0023424\n       _cons |    1.50274   .0244648    61.42   0.000      1.45479     1.55069\n------------------------------------------------------------------------------\nWe see the same sort of header information, including a Chi2 test for model significance and another pseudo \\(R^2\\).\nThe coefficients are again not interpretable other than sign and magnitude, but we can report incidence-rate ratios (IRR) instead. The irr option produces these, which are just the exponetiated coefficients.\n. poisson, irr\n\nPoisson regression                                      Number of obs =  4,231\n                                                        LR chi2(7)    = 986.09\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -8755.2185                             Pseudo R2     = 0.0533\n\n------------------------------------------------------------------------------\n    totrooms |        IRR   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ndollarel1000 |   1.059374   .0076921     7.94   0.000     1.044405    1.074558\n totsqft1000 |    1.11907   .0055375    22.73   0.000     1.108269    1.129976\n    1.cellar |   1.062855   .0169102     3.83   0.000     1.030223    1.096521\n             |\n     regionc |\n    Midwest  |   1.024525   .0204331     1.21   0.224     .9852499    1.065367\n      South  |   1.037601   .0219042     1.75   0.080     .9955457    1.081433\n       West  |   1.073145   .0234265     3.23   0.001     1.028198    1.120056\n             |\n      female |   .9795421   .0115011    -1.76   0.078     .9572578    1.002345\n       _cons |   4.493985   .1099446    61.42   0.000     4.283582    4.714722\n------------------------------------------------------------------------------\nNote: _cons estimates baseline incidence rate.\nIRR’s are slightly easier to interpret than OR’s. Each represents a average percent change in the count of the outcome predicted when there is a 1 increase in the predictor variable.\nFor example, the IRR for square footage is 1.119 which translates to a 11.9% predicted average increase in the number of rooms in a house which increases in sample size by 1,000 square feet.\n\n\n2.2.2 Interactions, categorical variables, margins, predict\nInteractions and categorical variables work the same. margins estimates the marginal means, which are the expected number of counts. predict by default predicts the number of events.\n\n\n2.2.3 Assumptions\nPoisson regression has the same independence assumption. It also has a very strong assumption which is specific to the Poisson distribution - namely that the mean and variance of a Poisson random variable are equal. In other words, as the mean of a Poisson variable increases, it becomes more spread out in a linear fashion.\nWe can examine whether this may be true for a given variable.\n. summarize totrooms\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n    totrooms |      5,686    6.191347    2.360918          1         19\nHere the mean, 6.19 is very close to the variance, 2.36\\(^2\\) = 5.57. This is not always the case for count data. If this is not true, a Negative Binomial model may be more appropriate. It’s extremely similar to Poisson, except it allows the mean and variance to be decoupled by means of \\(\\alpha\\), called the overdispersion factor.\nWe can fit it with the nbreg command.\n. nbreg totrooms dollarel1000 totsqft1000 i.cellar i.regionc female\n\nFitting Poisson model:\n\nIteration 0:  Log likelihood = -8755.2204  \nIteration 1:  Log likelihood = -8755.2185  \nIteration 2:  Log likelihood = -8755.2185  \n\nFitting constant-only model:\n\nIteration 0:  Log likelihood = -12716.439  \nIteration 1:  Log likelihood = -9248.2645  \nIteration 2:  Log likelihood = -9248.2645  \n\nFitting full model:\n\nIteration 0:  Log likelihood = -8758.8756  \nIteration 1:  Log likelihood = -8755.2187  \nIteration 2:  Log likelihood = -8755.2185  \n\nNegative binomial regression                            Number of obs =  4,231\n                                                        LR chi2(7)    = 986.09\nDispersion: mean                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -8755.2185                             Pseudo R2     = 0.0533\n\n------------------------------------------------------------------------------\n    totrooms | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ndollarel1000 |   .0576783    .007261     7.94   0.000      .043447    .0719096\n totsqft1000 |   .1124977   .0049483    22.73   0.000     .1027992    .1221962\n    1.cellar |   .0609588   .0159101     3.83   0.000     .0297756    .0921421\n             |\n     regionc |\n    Midwest  |   .0242295    .019944     1.21   0.224      -.01486    .0633189\n      South  |   .0369115   .0211105     1.75   0.080    -.0044642    .0782873\n       West  |   .0705931   .0218298     3.23   0.001     .0278075    .1133788\n             |\n      female |  -.0206701   .0117413    -1.76   0.078    -.0436825    .0023424\n       _cons |    1.50274   .0244648    61.42   0.000      1.45479     1.55069\n-------------+----------------------------------------------------------------\n    /lnalpha |  -27.77865          .                             .           .\n-------------+----------------------------------------------------------------\n       alpha |   8.63e-13          .                             .           .\n------------------------------------------------------------------------------\nLR test of alpha=0: chibar2(01) = 0.00                 Prob &gt;= chibar2 = 1.000\nThe row for alpha is the estimate of the overdispersion factor. If this value is close to 0, the Poisson model is appropriate. That’s what is occcurring here - in fact it’s so close to 0 that Stata refuses to even compute a standard error for it. The likelihood ratio test below it is formally testing whether the Poisson model is appropriate; here the p-value of 1.0 let’s us stick with Poisson. If this rejected, the negative binomial model is more appropriate. The negative binomial model is interpreted in the same fashion as Poisson.\n\n\n2.2.4 Exposure\nSometimes the count is limited by the exposure - for example, if you are counting the number of students in a class who fail an exam, this number will likely be higher for classes with more students. We can adjust for this in the Poisson model by specifying the exposure(___) option.\nIn the energy data, let’s say instead of the total number of rooms, we want to predict the number of bedrooms. Obviously the total number of rooms in the house will greatly affect the number of bedrooms.\n. histogram bedrooms\n(bin=37, start=0, width=.27027027)\n\n. poisson bedrooms dollarel1000 totsqft1000 i.cellar i.regionc female, exposure\n&gt; (totrooms) irr\n\nIteration 0:  Log likelihood = -6697.7375  \nIteration 1:  Log likelihood = -6697.7375  \n\nPoisson regression                                      Number of obs =  4,231\n                                                        LR chi2(7)    =  29.36\n                                                        Prob &gt; chi2   = 0.0001\nLog likelihood = -6697.7375                             Pseudo R2     = 0.0022\n\n------------------------------------------------------------------------------\n    bedrooms |        IRR   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ndollarel1000 |    1.00444   .0110315     0.40   0.687       .98305    1.026296\n totsqft1000 |   .9736346   .0074929    -3.47   0.001      .959059    .9884317\n    1.cellar |   .9749456   .0229176    -1.08   0.280      .931047    1.020914\n             |\n     regionc |\n    Midwest  |   .9819498   .0292231    -0.61   0.540      .926312    1.040929\n      South  |   .9932802   .0310743    -0.22   0.829     .9342054    1.056091\n       West  |   1.023995   .0329379     0.74   0.461     .9614307     1.09063\n             |\n      female |   1.000022    .017315     0.00   0.999     .9666552    1.034542\n       _cons |   .4965649   .0181979   -19.10   0.000     .4621485    .5335443\nln(totrooms) |          1  (exposure)\n------------------------------------------------------------------------------\nNote: _cons estimates baseline incidence rate.\nWe interpret this exactly as before. Negative binomial models can likewise have exposure(___) set."
  },
  {
    "objectID": "02-glm.html#other-regression-models",
    "href": "02-glm.html#other-regression-models",
    "title": "2  Generalized Linear Models",
    "section": "2.3 Other regression models",
    "text": "2.3 Other regression models\nThere are two extensions to logistic regression, ordinal logistic and multinomial logistic.\nOrdinal logistic is used when there are more than 2 outcome categories, and they are ordered (e.g. not sick (0), mildly sick (1), very sick (2)). Using ologit, Stata estimates an underlying continuous distribution and returns the “cut points”, allowing categorization.\nIf there are multiple groups but not ordered, e.g. race, use mlogit for multinomial logistic regression. It essentially fits a model predicting membership in each group versus all other, with some restrictions across the models."
  }
]